# base
seed: 0
deterministic: True

# environment config
device: [5]     # examples: [0], [0,1], [1,2,3], cpu, mps... 

# project config
project: outputs/mimic3
name: automatic_coding

# model config
model: llama3
bit: 8
max_len: 1024
pooling_mode: mean              # [mean, weighted_mean, bos_token, eos_token]
lora_activate: False

# data config
workers: 0                      # Don't worry to set worker. The number of workers will be set automatically according to the batch size.
mimic3_data_path: data/
chunk_size: 100000
label_pruning: True             # If True, only the required ICD codes will be used, otherwise all codes will be used.

# train config
batch_size: 16
steps: 30000
warmup_steps: 200
lr0: 1e-4
lrf: 0.1                              # last_lr = lr0 * lrf
scheduler_type: 'cosine'              # ['linear', 'cosine']
gradient_checkpointing: False
patience: 5                           # Early stopping epochs.
prediction_print_n: 10                # Number of examples to show during inference.
positive_threshold: 0.5

# logging config
common: ['train_loss', 'validation_loss', 'lr']
metrics: ['aucroc_macro', 'aucroc_micro', 'aucprc_macro', 'aucprc_micro']   # You can add more metrics after implements metric validation codes